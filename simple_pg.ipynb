{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simple_pg.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1f5A45yu10PY358V_je8N3cV82cCIaGUE","authorship_tag":"ABX9TyNK7tmuoGkAF+4BWdfJg4uk"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bduJUGARl1j6","colab_type":"text"},"source":["#### The simplest formulation of policy gradient with real-time rendering\n","\n","You can find more details about implementation [here](https://spinningup.openai.com/en/latest/user/introduction.html)."]},{"cell_type":"code","metadata":{"id":"w76gYToekwFp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600245854111,"user_tz":-540,"elapsed":16275,"user":{"displayName":"Yamato Kataoka","photoUrl":"","userId":"11097519179436224576"}}},"source":["!apt-get install -y xvfb \\\n","                    python-opengl > /dev/null\n","!pip install gym \\\n","             pyvirtualdisplay > /dev/null"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"So1Vq45ET39O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600247195484,"user_tz":-540,"elapsed":733,"user":{"displayName":"Yamato Kataoka","photoUrl":"","userId":"11097519179436224576"}},"outputId":"be53f297-a2ff-460d-9a5b-2c905fbb39c1"},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(400, 300))\n","display.start()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f5a7ec14c50>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"NUc75kACn_I9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":507},"outputId":"938abc89-43c6-4e8c-b4ba-515696d807b2"},"source":["%matplotlib inline\n","import torch\n","import torch.nn as nn\n","from torch.distributions.categorical import Categorical\n","from torch.optim import Adam\n","import numpy as np\n","import gym\n","from gym.spaces import Discrete, Box\n","\n","from IPython import display as ipythondisplay\n","import matplotlib.pyplot as plt\n","\n","def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n","    # Build a feedforward neural network.\n","    layers = []\n","    for j in range(len(sizes)-1):\n","      act = activation if j < len(sizes)-2 else output_activation\n","      layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n","    return nn.Sequential(*layers)\n","\n","def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n","          epochs=50, batch_size=5000, render=False):\n","\n","    # make environment, check spaces, get obs / act dims\n","    env = gym.make(env_name)\n","    assert isinstance(env.observation_space, Box), \\\n","        \"This example only works for envs with continuous state spaces.\"\n","    assert isinstance(env.action_space, Discrete), \\\n","        \"This example only works for envs with discrete action spaces.\"\n","\n","    obs_dim = env.observation_space.shape[0]\n","    n_acts = env.action_space.n\n","\n","    # make core of policy network\n","    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n","\n","    # make function to compute action distribution\n","    def get_policy(obs):\n","      logits = logits_net(obs)\n","      return Categorical(logits=logits)\n","\n","    # make action selection function (outputs int actions, sampled from policy)\n","    def get_action(obs):\n","      return get_policy(obs).sample().item()\n","\n","    # make loss function whose gradient, for the right data, is policy gradient\n","    def compute_loss(obs, act, weights):\n","      logp = get_policy(obs).log_prob(act)\n","      return -(logp * weights).mean()\n","\n","    # make optimizer\n","    optimizer = Adam(logits_net.parameters(), lr=lr)\n","\n","    # for training policy\n","    def train_one_epoch():\n","      # make some empty lists for logging.\n","      batch_obs = []          # for observations\n","      batch_acts = []         # for actions\n","      batch_weights = []      # for R(tau) weighting in policy gradient\n","      batch_rets = []         # for measuring episode returns\n","      batch_lens = []         # for measuring episode lengths\n","\n","      # reset episode-specific variables\n","      obs = env.reset()       # first obs comes from starting distribution\n","      done = False            # signal from environment that episode is over\n","      ep_rews = []            # list for rewards accrued throughout ep\n","\n","      # render first episode of each epoch\n","      finished_rendering_this_epoch = False\n","\n","      # collect experience by acting in the environment with current policy\n","      while True:\n","\n","        # rendering\n","        if (not finished_rendering_this_epoch) and render:\n","          screen = env.render(mode='rgb_array')\n","\n","          plt.imshow(screen)\n","          ipythondisplay.clear_output(wait=True)\n","          ipythondisplay.display(plt.gcf())\n","\n","        # print epoch infomation\n","        if (not finished_rendering_this_epoch) and epoch_logs:\n","          print('epoch logs\\n')\n","          for epoch_info in epoch_logs:\n","            print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n","                (epoch_info['epoch'], epoch_info['batch_loss'], np.mean(epoch_info['batch_rets']), np.mean(epoch_info['batch_lens'])))\n","\n","        # save obs\n","        batch_obs.append(obs.copy())\n","\n","        # act in the environment\n","        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n","        obs, rew, done, _ = env.step(act)\n","\n","        # save action, reward\n","        batch_acts.append(act)\n","        ep_rews.append(rew)\n","\n","        if done:\n","          # if episode is over, record info about episode\n","          ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n","          batch_rets.append(ep_ret)\n","          batch_lens.append(ep_len)\n","\n","          # the weight for each logprob(a|s) is R(tau)\n","          batch_weights += [ep_ret] * ep_len\n","\n","          # reset episode-specific variables\n","          obs, done, ep_rews = env.reset(), False, []\n","\n","          # won't render again this epoch\n","          finished_rendering_this_epoch = True\n","\n","          # end experience loop if we have enough of it\n","          if len(batch_obs) > batch_size:\n","            break\n","\n","      # take a single policy gradient update step\n","      optimizer.zero_grad()\n","      batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n","                                act=torch.as_tensor(batch_acts, dtype=torch.int32),\n","                                weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n","                                )\n","      batch_loss.backward()\n","      optimizer.step()\n","      return batch_loss, batch_rets, batch_lens\n","\n","    epoch_logs = []\n","\n","    # training loop\n","    for i in range(epochs):\n","        batch_loss, batch_rets, batch_lens = train_one_epoch()\n","        epoch_info = {\"epoch\": i, \"batch_loss\": batch_loss, \"batch_rets\": batch_rets, \"batch_lens\": batch_lens}\n","        epoch_logs.append(epoch_info)\n","\n","print('\\nUsing simplest formulation of policy gradient.\\n')\n","train(render=True)\n","\n","ipythondisplay.clear_output(wait=True)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch:   0 \t loss: 17.152 \t return: 20.941 \t ep_len: 20.941\n","epoch:   1 \t loss: 18.211 \t return: 22.031 \t ep_len: 22.031\n","epoch:   2 \t loss: 22.243 \t return: 25.280 \t ep_len: 25.280\n","epoch:   3 \t loss: 24.347 \t return: 27.348 \t ep_len: 27.348\n","epoch:   4 \t loss: 24.704 \t return: 28.129 \t ep_len: 28.129\n","epoch:   5 \t loss: 30.823 \t return: 32.791 \t ep_len: 32.791\n","epoch:   6 \t loss: 29.093 \t return: 34.322 \t ep_len: 34.322\n","epoch:   7 \t loss: 31.899 \t return: 38.527 \t ep_len: 38.527\n","epoch:   8 \t loss: 38.864 \t return: 45.582 \t ep_len: 45.582\n","epoch:   9 \t loss: 40.967 \t return: 47.217 \t ep_len: 47.217\n","epoch:  10 \t loss: 46.884 \t return: 57.023 \t ep_len: 57.023\n","epoch:  11 \t loss: 42.733 \t return: 53.660 \t ep_len: 53.660\n","epoch:  12 \t loss: 42.242 \t return: 54.467 \t ep_len: 54.467\n","epoch:  13 \t loss: 51.583 \t return: 67.581 \t ep_len: 67.581\n"],"name":"stdout"}]}]}